{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Methods for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "# if you are using the fastAI environment, all of these imports work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None: self.total = tsize\n",
    "        self.update(b * bsize - self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, filename):\n",
    "    \"\"\"\n",
    "    Download data if the filename does not exist already\n",
    "    Uses Tqdm to show download progress\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "            urlretrieve(url, filename, reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download some data:\n",
    "data_url = 'http://files.fast.ai/data/aclImdb.tgz'\n",
    "# get_data(data_url, 'data/imdb.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, *manually extract the files* please!\n",
    "The *.tgz* extension is equivalent to *.tar.gz* here. \n",
    "\n",
    "On Windows, you might need a software like *7z* \n",
    "On Linux, you can probably use *tar -xvcf imdb.tgz* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.getcwd())/'data'/'aclImdb'\n",
    "assert data_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to check that we have extracted the files at the correct location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "all\n",
      "neg\n",
      "pos\n",
      "all\n",
      "neg\n",
      "pos\n",
      "unsup\n"
     ]
    }
   ],
   "source": [
    "for pathroute in os.walk(data_path):\n",
    "    next_path = pathroute[1]\n",
    "    for stop in next_path:\n",
    "        print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path/'train'\n",
    "test_path = data_path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir_path):\n",
    "    \"\"\"read data into pandas dataframe\"\"\"\n",
    "    \n",
    "    def load_dir_reviews(reviews_path):\n",
    "        files_list = list(reviews_path.iterdir())\n",
    "        reviews = []\n",
    "        for filename in files_list:\n",
    "            f = open(filename, 'r', encoding='utf-8')\n",
    "            reviews.append(f.read())\n",
    "        return pd.DataFrame({'text':reviews})\n",
    "        \n",
    "    \n",
    "    pos_path = dir_path/'pos'\n",
    "    neg_path = dir_path/'neg'\n",
    "    \n",
    "    pos_reviews, neg_reviews = load_dir_reviews(pos_path), load_dir_reviews(neg_path)\n",
    "    \n",
    "    pos_reviews['label'] = 1\n",
    "    neg_reviews['label'] = 0\n",
    "    \n",
    "    merged = pd.concat([pos_reviews, neg_reviews])\n",
    "    df = merged.sample(frac=1.0) # shuffle the rows\n",
    "    df.reset_index(inplace=True) # don't carry index from previous\n",
    "    df.drop(columns=['index'], inplace=True) # drop the column 'index' \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path/'train'\n",
    "test_path = data_path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = read_data(train_path)\n",
    "test = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, I'm a few days late but what the hell......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just watched this and it was amazing. Was in s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A hundred miles away from the scene of a grizz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a case where the script plays with the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final Draft - A screenwriter (James Van Der Be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Well, I'm a few days late but what the hell......      1\n",
       "1  Just watched this and it was amazing. Was in s...      1\n",
       "2  A hundred miles away from the scene of a grizz...      1\n",
       "3  This is a case where the script plays with the...      1\n",
       "4  Final Draft - A screenwriter (James Van Der Be...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv(data_path/'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(data_path/'train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['text'], train['label']\n",
    "X_test, y_test = test['text'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf.fit(X=X_train, y=y_train) # note that .fit function calls are inplace, and the Pipeline is not re-assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predicted = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88316"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_acc = sum(lr_predicted == y_test)/len(lr_predicted)\n",
    "lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_acc(pipeline_clf):\n",
    "    predictions = pipeline_clf.predict(X_test)\n",
    "    assert len(y_test) == len(predictions)\n",
    "    return sum(predictions == y_test)/len(y_test), predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',LR())])\n",
    "lr_clf.fit(X=X_train, y=y_train)\n",
    "lr_acc, lr_predictions = imdb_acc(lr_clf)\n",
    "lr_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the Ngram Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',LR())])\n",
    "lr_clf.fit(X=X_train, y=y_train)\n",
    "lr_acc, lr_predictions = imdb_acc(lr_clf)\n",
    "lr_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "mnb_clf = Pipeline([('vect', CountVectorizer()), ('clf',MNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81356"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "mnb_acc, mnb_predictions = imdb_acc(mnb_clf)\n",
    "mnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82956"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',MNB())])\n",
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "mnb_acc, mnb_predictions = imdb_acc(mnb_clf)\n",
    "mnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82992"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',MNB())])\n",
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "mnb_acc, mnb_predictions = imdb_acc(mnb_clf)\n",
    "mnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Ngram Range from 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB())])\n",
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "mnb_acc, mnb_predictions = imdb_acc(mnb_clf)\n",
    "mnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Fit Prior to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB(fit_prior=False))])\n",
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "mnb_acc, mnb_predictions = imdb_acc(mnb_clf)\n",
    "mnb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',SVC())])\n",
    "svc_clf.fit(X=X_train, y=y_train)\n",
    "svc_acc, svc_predictions = imdb_acc(svc_clf)\n",
    "print(svc_acc) # 0.6562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Baseed Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7068"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "dtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',DTC())])\n",
    "dtc_clf.fit(X=X_train, y=y_train)\n",
    "dtc_acc, dtc_predictions = imdb_acc(dtc_clf)\n",
    "dtc_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73472"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "rfc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',RFC())])\n",
    "rfc_clf.fit(X=X_train, y=y_train)\n",
    "rfc_acc, rfc_predictions = imdb_acc(rfc_clf)\n",
    "rfc_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75096"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier as XTC\n",
    "xtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',XTC())])\n",
    "xtc_clf.fit(X=X_train, y=y_train)\n",
    "xtc_acc, xtc_predictions = imdb_acc(xtc_clf)\n",
    "xtc_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically Fine Tuning \n",
    "\n",
    "### RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = dict(clf__C=[50, 75, 85, 100], \n",
    "                  vect__stop_words=['english', None],\n",
    "                  vect__ngram_range = [(1, 1), (1, 3)],\n",
    "                  vect__lowercase = [True, False],\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=-1,\n",
       "          param_distributions={'clf__C': [50, 75, 85, 100], 'vect__stop_words': ['english', None], 'vect__ngram_range': [(1, 1), (1, 3)], 'vect__lowercase': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(lr_clf, param_distributions=param_grid, n_iter=5, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cross-validation accuracy: 0.89884\n"
     ]
    }
   ],
   "source": [
    "print(f'Calculated cross-validation accuracy: {random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_clf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90096, array([1, 1, 0, ..., 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_acc(best_random_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('tfidf',\n",
       "  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('clf',\n",
       "  LogisticRegression(C=75, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_clf.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(clf__C=[85, 100, 125, 150])\n",
    "grid_search = GridSearchCV(lr_clf, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibMemoryError",
     "evalue": "JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001B7102B10C0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-36.pyc', '__doc__': None, '__file__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'D:\\\\Minicond...lp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001B7102B10C0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-36.pyc', '__doc__': None, '__file__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'D:\\\\Minicond...lp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py in <module>()\n      1 if __name__ == '__main__':\n      2     from ipykernel import kernelapp as app\n----> 3     app.launch_new_instance()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    122         except (RuntimeError, AssertionError):\n    123             old_loop = None\n    124         try:\n    125             self._setup_logging()\n    126             asyncio.set_event_loop(self.asyncio_loop)\n--> 127             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Win...EventLoop running=True closed=False debug=False>>\n    128         finally:\n    129             asyncio.set_event_loop(old_loop)\n    130 \n    131     def stop(self):\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py in run_forever(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_Windo...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py in _run_once(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n   1429                         logger.warning('Executing %s took %.3f seconds',\n   1430                                        _format_handle(handle), dt)\n   1431                 finally:\n   1432                     self._current_handle = None\n   1433             else:\n-> 1434                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop._run_callback(functools.par...01B7150CA268>))>>\n   1435         handle = None  # Needed to break cycles when an exception occurs.\n   1436 \n   1437     def _set_coroutine_wrapper(self, enabled):\n   1438         try:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\events.py in _run(self=<Handle IOLoop._run_callback(functools.par...01B7150CA268>))>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method IOLoop._run_callback of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>),)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>))\n    754         \"\"\"Runs a callback with error handling.\n    755 \n    756         For use in subclasses.\n    757         \"\"\"\n    758         try:\n--> 759             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>)\n    760             if ret is not None:\n    761                 from tornado import gen\n    762                 # Functions that return Futures typically swallow all\n    763                 # exceptions and store them in the Future.  If a Future\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ()\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in <lambda>()\n    531             return\n    532 \n    533         if state & self.socket.events:\n    534             # events still exist that haven't been processed\n    535             # explicitly schedule handling to avoid missing events due to edge-triggered FDs\n--> 536             self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n    537 \n    538     def _init_io_state(self):\n    539         \"\"\"initialize the ioloop event handler\"\"\"\n    540         with stack_context.NullContext():\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=0)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'a6f2e7c3cb634e10b16d91097af28c58']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'a6f2e7c3cb634e10b16d91097af28c58'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='%%time\\ngrid_search.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '%%time\\ngrid_search.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('%%time\\ngrid_search.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('%%time\\ngrid_search.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='%%time\\ngrid_search.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = '%%time\\ngrid_search.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='%%time\\ngrid_search.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-44-9855dc574354>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>)\n   2904                     return True\n   2905 \n   2906             for i, node in enumerate(to_run_interactive):\n   2907                 mod = ast.Interactive([node])\n   2908                 code = compiler(mod, cell_name, \"single\")\n-> 2909                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>\n        result = <ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>\n   2910                     return True\n   2911 \n   2912             # Flush softspace\n   2913             if softspace(sys.stdout, 0):\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>, result=<ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>)\n   2958         outflag = True  # happens in more places, so it's easier as default\n   2959         try:\n   2960             try:\n   2961                 self.hooks.pre_run_code_hook()\n   2962                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2963                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n   2964             finally:\n   2965                 # Reset our crash handler in place\n   2966                 sys.excepthook = old_excepthook\n   2967         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<ipython-input-44-9855dc574354> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', 'grid_search.fit(X_train, y_train)')\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell='grid_search.fit(X_train, y_train)')\n   2162             # This will need to be updated if the internal calling logic gets\n   2163             # refactored, or else we'll be expanding the wrong variables.\n   2164             stack_depth = 2\n   2165             magic_arg_s = self.var_expand(line, stack_depth)\n   2166             with self.builtin_trap:\n-> 2167                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = 'grid_search.fit(X_train, y_train)'\n   2168             return result\n   2169 \n   2170     def find_line_magic(self, magic_name):\n   2171         \"\"\"Find and return a line magic by name.\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<decorator-gen-63> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell='grid_search.fit(X_train, y_train)', local_ns=None)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', 'grid_search.fit(X_train, y_train)', None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', 'grid_search.fit(X_train, y_train)', None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magics\\execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell='grid_search.fit(X_train, y_train)', local_ns=None)\n   1225         # time execution\n   1226         wall_st = wtime()\n   1227         if mode=='eval':\n   1228             st = clock2()\n   1229             try:\n-> 1230                 out = eval(code, glob, local_ns)\n        out = undefined\n        code = <code object <module> at 0x000001B72D7D24B0, file \"<timed eval>\", line 1>\n        glob = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n        local_ns = None\n   1231             except:\n   1232                 self.shell.showtraceback()\n   1233                 return\n   1234             end = clock2()\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<timed eval> in <module>()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=3, error_score='raise',\n       e...ore='warn', scoring='accuracy',\n       verbose=0), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, groups=None, **fit_params={})\n    635                                   return_train_score=self.return_train_score,\n    636                                   return_n_test_samples=True,\n    637                                   return_times=True, return_parameters=False,\n    638                                   error_score=self.error_score)\n    639           for parameters, (train, test) in product(candidate_params,\n--> 640                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64\n        groups = None\n    641 \n    642         # if one choose to see train score, \"out\" will contain train score info\n    643         if self.return_train_score:\n    644             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Wed Sep 26 05:47:48 2018\nPID: 4472                   Python 3.6.6: D:\\Miniconda3\\envs\\nlp\\python.exe\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, scorer={'score': make_scorer(accuracy_score)}, train=array([    0,     1,     2, ..., 16685, 16686, 16687]), test=array([16641, 16643, 16644, ..., 24997, 24998, 24999]), verbose=0, parameters={'clf__C': 85}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No....0001,\n          verbose=0, warm_start=False))])>\n        X_train = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y_train = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N....0001,\n          verbose=0, warm_start=False))])>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x000001D3E9EC6158>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        vocabulary = <class 'dict'> instance\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _sort_features(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), X=<16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, vocabulary=<class 'dict'> instance)\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([      0,       1,       2, ..., 3696111, 3696112, 3696113],\n      dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py\", line 248, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py\", line 213, in _fit\n    **fit_params_steps[name])\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 362, in __call__\n    return self.func(*args, **kwargs)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py\", line 581, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 875, in fit_transform\n    X = self._sort_features(X, vocabulary)\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 731, in _sort_features\n    X.indices = map_index.take(X.indices, mode='clip')\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Wed Sep 26 05:47:48 2018\nPID: 4472                   Python 3.6.6: D:\\Miniconda3\\envs\\nlp\\python.exe\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, scorer={'score': make_scorer(accuracy_score)}, train=array([    0,     1,     2, ..., 16685, 16686, 16687]), test=array([16641, 16643, 16644, ..., 24997, 24998, 24999]), verbose=0, parameters={'clf__C': 85}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No....0001,\n          verbose=0, warm_start=False))])>\n        X_train = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y_train = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N....0001,\n          verbose=0, warm_start=False))])>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x000001D3E9EC6158>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        vocabulary = <class 'dict'> instance\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _sort_features(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), X=<16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, vocabulary=<class 'dict'> instance)\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([      0,       1,       2, ..., 3696111, 3696112, 3696113],\n      dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\nlp\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Wed Sep 26 05:47:48 2018\nPID: 4472                   Python 3.6.6: D:\\Miniconda3\\envs\\nlp\\python.exe\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, scorer={'score': make_scorer(accuracy_score)}, train=array([    0,     1,     2, ..., 16685, 16686, 16687]), test=array([16641, 16643, 16644, ..., 24997, 24998, 24999]), verbose=0, parameters={'clf__C': 85}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No....0001,\n          verbose=0, warm_start=False))])>\n        X_train = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y_train = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N....0001,\n          verbose=0, warm_start=False))])>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x000001D3E9EC6158>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        vocabulary = <class 'dict'> instance\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _sort_features(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), X=<16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, vocabulary=<class 'dict'> instance)\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([      0,       1,       2, ..., 3696111, 3696112, 3696113],\n      dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 640\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibMemoryError\u001b[0m: JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001B7102B10C0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-36.pyc', '__doc__': None, '__file__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'D:\\\\Minicond...lp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001B7102B10C0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-36.pyc', '__doc__': None, '__file__': r'D:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...nlp\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'D:\\\\Minicond...lp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py in <module>()\n      1 if __name__ == '__main__':\n      2     from ipykernel import kernelapp as app\n----> 3     app.launch_new_instance()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    122         except (RuntimeError, AssertionError):\n    123             old_loop = None\n    124         try:\n    125             self._setup_logging()\n    126             asyncio.set_event_loop(self.asyncio_loop)\n--> 127             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Win...EventLoop running=True closed=False debug=False>>\n    128         finally:\n    129             asyncio.set_event_loop(old_loop)\n    130 \n    131     def stop(self):\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py in run_forever(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n    417             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    418                                    finalizer=self._asyncgen_finalizer_hook)\n    419         try:\n    420             events._set_running_loop(self)\n    421             while True:\n--> 422                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_Windo...EventLoop running=True closed=False debug=False>>\n    423                 if self._stopping:\n    424                     break\n    425         finally:\n    426             self._stopping = False\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py in _run_once(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n   1429                         logger.warning('Executing %s took %.3f seconds',\n   1430                                        _format_handle(handle), dt)\n   1431                 finally:\n   1432                     self._current_handle = None\n   1433             else:\n-> 1434                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop._run_callback(functools.par...01B7150CA268>))>>\n   1435         handle = None  # Needed to break cycles when an exception occurs.\n   1436 \n   1437     def _set_coroutine_wrapper(self, enabled):\n   1438         try:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\asyncio\\events.py in _run(self=<Handle IOLoop._run_callback(functools.par...01B7150CA268>))>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method IOLoop._run_callback of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>),)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>))\n    754         \"\"\"Runs a callback with error handling.\n    755 \n    756         For use in subclasses.\n    757         \"\"\"\n    758         try:\n--> 759             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function wrap.<locals>.null_wrapper at 0x000001B7150CA268>)\n    760             if ret is not None:\n    761                 from tornado import gen\n    762                 # Functions that return Futures typically swallow all\n    763                 # exceptions and store them in the Future.  If a Future\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ()\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in <lambda>()\n    531             return\n    532 \n    533         if state & self.socket.events:\n    534             # events still exist that haven't been processed\n    535             # explicitly schedule handling to avoid missing events due to edge-triggered FDs\n--> 536             self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n    537 \n    538     def _init_io_state(self):\n    539         \"\"\"initialize the ioloop event handler\"\"\"\n    540         with stack_context.NullContext():\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=0)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'a6f2e7c3cb634e10b16d91097af28c58']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'a6f2e7c3cb634e10b16d91097af28c58'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '%%time\\ngrid_search.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 25, 23, 40, 27, 639848, tzinfo=tzutc()), 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'session': 'a6f2e7c3cb634e10b16d91097af28c58', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '0a644ca9c93a422b8ffb37b847a55c64', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='%%time\\ngrid_search.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '%%time\\ngrid_search.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('%%time\\ngrid_search.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('%%time\\ngrid_search.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='%%time\\ngrid_search.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = '%%time\\ngrid_search.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='%%time\\ngrid_search.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-44-9855dc574354>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>)\n   2904                     return True\n   2905 \n   2906             for i, node in enumerate(to_run_interactive):\n   2907                 mod = ast.Interactive([node])\n   2908                 code = compiler(mod, cell_name, \"single\")\n-> 2909                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>\n        result = <ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>\n   2910                     return True\n   2911 \n   2912             # Flush softspace\n   2913             if softspace(sys.stdout, 0):\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>, result=<ExecutionResult object at 1b72d7d5e48, executio...rue silent=False shell_futures=True> result=None>)\n   2958         outflag = True  # happens in more places, so it's easier as default\n   2959         try:\n   2960             try:\n   2961                 self.hooks.pre_run_code_hook()\n   2962                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2963                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001B7176F58A0, file \"<ipython-input-44-9855dc574354>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n   2964             finally:\n   2965                 # Reset our crash handler in place\n   2966                 sys.excepthook = old_excepthook\n   2967         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<ipython-input-44-9855dc574354> in <module>()\n----> 1 get_ipython().run_cell_magic('time', '', 'grid_search.fit(X_train, y_train)')\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell_magic(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, magic_name='time', line='', cell='grid_search.fit(X_train, y_train)')\n   2162             # This will need to be updated if the internal calling logic gets\n   2163             # refactored, or else we'll be expanding the wrong variables.\n   2164             stack_depth = 2\n   2165             magic_arg_s = self.var_expand(line, stack_depth)\n   2166             with self.builtin_trap:\n-> 2167                 result = fn(magic_arg_s, cell)\n        result = undefined\n        fn = <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object>>\n        magic_arg_s = ''\n        cell = 'grid_search.fit(X_train, y_train)'\n   2168             return result\n   2169 \n   2170     def find_line_magic(self, magic_name):\n   2171         \"\"\"Find and return a line magic by name.\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<decorator-gen-63> in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell='grid_search.fit(X_train, y_train)', local_ns=None)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magic.py in <lambda>(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, '', 'grid_search.fit(X_train, y_train)', None), **k={})\n    182     validate_type(magic_kind)\n    183 \n    184     # This is a closure to capture the magic_kind.  We could also use a class,\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--> 187         call = lambda f, *a, **k: f(*a, **k)\n        f = <function ExecutionMagics.time>\n        a = (<IPython.core.magics.execution.ExecutionMagics object>, '', 'grid_search.fit(X_train, y_train)', None)\n        k = {}\n    188 \n    189         if callable(arg):\n    190             # \"Naked\" decorator call (just @foo, no args)\n    191             func = arg\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magics\\execution.py in time(self=<IPython.core.magics.execution.ExecutionMagics object>, line='', cell='grid_search.fit(X_train, y_train)', local_ns=None)\n   1225         # time execution\n   1226         wall_st = wtime()\n   1227         if mode=='eval':\n   1228             st = clock2()\n   1229             try:\n-> 1230                 out = eval(code, glob, local_ns)\n        out = undefined\n        code = <code object <module> at 0x000001B72D7D24B0, file \"<timed eval>\", line 1>\n        glob = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DTC': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'from pathlib import Path\\nimport pandas as pd\\nimp...the fastAI environment, all of these imports work', 'class TqdmUpTo(tqdm):\\n    def update_to(self, b=...l = tsize\\n        self.update(b * bsize - self.n)', 'def get_data(url, filename):\\n    \"\"\"\\n    Downloa...rlretrieve(url, filename, reporthook=t.update_to)', \"# Let's download some data:\\ndata_url = 'http://f...clImdb.tgz'\\n# get_data(data_url, 'data/imdb.tgz')\", \"data_path = Path(os.getcwd())/'data'/'aclImdb'\\nassert data_path.exists()\", 'for pathroute in os.walk(data_path):\\n    next_pa...1]\\n    for stop in next_path:\\n        print(stop)', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", 'def read_data(dir_path):\\n    \"\"\"read data into p...ce=True) # drop the column \\'index\\' \\n    return df', \"train_path = data_path/'train'\\ntest_path = data_path/'test'\", r\"get_ipython().run_cell_magic('time', '', 'train ...d_data(train_path)\\ntest = read_data(test_path)')\", 'test[:5]', \"# test.to_csv(data_path/'test.csv', index=False)\", \"# train.to_csv(data_path/'train.csv', index=False)\", \"X_train, y_train = train['text'], train['label']\\nX_test, y_test = test['text'], test['label']\", 'from sklearn.pipeline import Pipeline\\nfrom sklea...ion.text import CountVectorizer, TfidfTransformer', 'from sklearn.linear_model import LogisticRegression as LR', \"lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])\", \"get_ipython().run_cell_magic('time', '', 'lr_clf...re inplace, and the Pipeline is not re-assigned')\", 'lr_predicted = lr_clf.predict(X_test)', ...], 'LR': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'Out': {11:                                                 ... one of my favorite movies ever! along ...      1, 18: Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 20: 0.88312, 22: 0.879, 23: 0.86596, 25: 0.81356, 26: 0.82956, 27: 0.82992, 28: 0.8572, 29: 0.8572, ...}, 'Path': <class 'pathlib.Path'>, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RFC': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, ...}\n        local_ns = None\n   1231             except:\n   1232                 self.shell.showtraceback()\n   1233                 return\n   1234             end = clock2()\n\n...........................................................................\nC:\\Users\\nirantk\\Desktop\\nlp-python-deep-learning\\<timed eval> in <module>()\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=3, error_score='raise',\n       e...ore='warn', scoring='accuracy',\n       verbose=0), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, groups=None, **fit_params={})\n    635                                   return_train_score=self.return_train_score,\n    636                                   return_n_test_samples=True,\n    637                                   return_times=True, return_parameters=False,\n    638                                   error_score=self.error_score)\n    639           for parameters, (train, test) in product(candidate_params,\n--> 640                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64\n        groups = None\n    641 \n    642         # if one choose to see train score, \"out\" will contain train score info\n    643         if self.return_train_score:\n    644             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Wed Sep 26 05:47:48 2018\nPID: 4472                   Python 3.6.6: D:\\Miniconda3\\envs\\nlp\\python.exe\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), 0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, {'score': make_scorer(accuracy_score)}, array([    0,     1,     2, ..., 16685, 16686, 16687]), array([16641, 16643, 16644, ..., 24997, 24998, 24999]), 0, {'clf__C': 85})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...lbert...\nName: text, Length: 25000, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...999    1\nName: label, Length: 25000, dtype: int64, scorer={'score': make_scorer(accuracy_score)}, train=array([    0,     1,     2, ..., 16685, 16686, 16687]), test=array([16641, 16643, 16644, ..., 24997, 24998, 24999]), verbose=0, parameters={'clf__C': 85}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No....0001,\n          verbose=0, warm_start=False))])>\n        X_train = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y_train = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N....0001,\n          verbose=0, warm_start=False))])>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...0.0001,\n          verbose=0, warm_start=False))]), X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x000001D3E9EC6158>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64, **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = 0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object\n        y = 0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=0        Antitrust falls right into that categor...'t ac...\nName: text, Length: 16668, dtype: object, y=0        0\n1        0\n2        1\n3        1\n4   ...687    1\nName: label, Length: 16668, dtype: int64)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        vocabulary = <class 'dict'> instance\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\nD:\\Miniconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _sort_features(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), X=<16668x3696114 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, vocabulary=<class 'dict'> instance)\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([      0,       1,       2, ..., 3696111, 3696112, 3696113],\n      dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ec1bd0e775ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "grid_search.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Calculated cross-validation accuracy: {grid_search.best_score_} while random_search was {random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_grid_clf = grid_search.best_estimator_\n",
    "best_grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_acc(best_grid_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "### Simple Majority (aka Hard Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voting_clf = VotingClassifier(estimators=[('xtc', xtc_clf), ('rfc', rfc_clf)], voting='hard', n_jobs=-1)\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_voting_acc, _ = imdb_acc(voting_clf)\n",
    "hard_voting_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('mnb', mnb_clf)], voting='soft', n_jobs=-1)\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_acc, _ = imdb_acc(voting_clf)\n",
    "soft_voting_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_acc = soft_voting_acc - lr_acc\n",
    "if gain_acc > 0:\n",
    "    print(f'We see that the soft voting gives us an absolute accuracy gain of {gain_acc*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weighted_voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('lr2', lr_clf),('rf', xtc_clf), ('mnb2', mnb_clf),('mnb', mnb_clf)], voting='soft', n_jobs=-1)\n",
    "weighted_voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment with 'hard' voting instead of 'soft' voting. This will tell you how does the voting strategy influence the accuracy of our ensembled classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_voting_acc, _ = imdb_acc(weighted_voting_clf)\n",
    "weighted_voting_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_acc = weighted_voting_acc - lr_acc\n",
    "if gain_acc > 0:\n",
    "    print(f'We see that the weighted voting gives us an absolute accuracy gain of {gain_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(mnb_predictions, lr_predictions)[0][1] # this is too high a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corr_voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('mnb', mnb_clf)], voting='soft', n_jobs=-1)\n",
    "corr_voting_clf.fit(X_train, y_train)\n",
    "corr_acc, _ = imdb_acc(corr_voting_clf)\n",
    "print(corr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(dtc_predictions,xtc_predictions )[0][1] # this is looks like a low correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "low_corr_voting_clf = VotingClassifier(estimators=[('dtc', dtc_clf), ('xtc', xtc_clf)], voting='soft', n_jobs=-1)\n",
    "low_corr_voting_clf.fit(X_train, y_train)\n",
    "low_corr_acc, _ = imdb_acc(low_corr_voting_clf)\n",
    "print(low_corr_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
