{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going ahead, first, let us understand the basics. What is a function in mathematics?\n",
    "A function represents the relation between input and output. We generally use $f$ to\n",
    "denote the function. For instance, $f(x) = x^2$ implies a function which takes as $x$ an\n",
    "input and returns $x^2$ as an output. It can also be represented as $ y = x^2$.\n",
    "So, we have a function $ y = x^2$ , we can plot and see what our function looks like:\n",
    "\n",
    "\n",
    "![image](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A minimum of the function is called the smallest value of a function. Like as you can see in\n",
    "the above figure, a minimum of the  $x^2$ function lies at 0. The above function is called a\n",
    "convex function where we have only one minimum value. A function is called\n",
    "a nonconvex function where there is more than one minimum value. As shown in the\n",
    "following figure, a nonconvex function can have many local minima and one global\n",
    "minimum value whereas a convex function has only one global minimum value. \n",
    "\n",
    "![image](images/2.png)\n",
    "\n",
    "By looking at the graph of $x^2$ function we easily said that it has its minimum value at $x=0$\n",
    ". But how can we find the minimum value of a function mathematically? First, let us\n",
    "assume, x=0.7 Thus, we are at a position where x = 0.7 as shown in the following graph. \n",
    "\n",
    "![image](images/3.png)\n",
    "\n",
    "\n",
    "Now we need to go to zero, which is our minimum value, but how can reach there? We can\n",
    "reach there by calculating the derivative of the function $y=x^2$. So derivative of the\n",
    "function $y$ with respect to $x$ is given as,\n",
    "\n",
    "\n",
    "$$y = x^2$$\n",
    "\n",
    "$$ \\frac{d y}{d x}=2 x $$\n",
    "\n",
    "As we are at x=0.7, substituting this in the above equation, we get,\n",
    "\n",
    "$$\\frac{d y}{d x}=2(0.7)=1.4$$\n",
    "\n",
    "\n",
    "After calculating the derivative, we update our position of $x$ by the following update rule:\n",
    "\n",
    "$$x=x-\\frac{d y}{d x}$$\n",
    "\n",
    "\n",
    "$$x=0.7-1.4$$\n",
    "\n",
    "$$ x=-0.7$$\n",
    "\n",
    "\n",
    "As shown in the below figure, first we were at x=0.7 but after computing gradient, we are\n",
    "now at the updated position x =-0.7. But this is something we don't want. Because we\n",
    "missed our minimum value which is x=0 and reached somewhere else.\n",
    "\n",
    "![image](images/4.png)\n",
    "\n",
    "\n",
    "So, to avoid this we introduce a new parameter called learning rate $\\alpha$. It helps us to slow\n",
    "down our gradient steps so that we won't miss out the minimal point. We multiply our\n",
    "gradients by the learning rate and update the $x$ value as,\n",
    "\n",
    "$$x=x-\\alpha \\frac{d y}{d x}$$\n",
    "\n",
    "\n",
    "Let us say $\\alpha = 0.15$ then,\n",
    "\n",
    "$$x=0.7-(0.15 * 1.4)$$\n",
    "\n",
    "$$x=0.49$$\n",
    "\n",
    "As shown in the below figure, after multiplying the gradients with learning rate, with the\n",
    "updated x value, we descended from the initial position x=0.7 to x =0.49.\n",
    "\n",
    "![image](images/5.png)\n",
    "\n",
    "But still, this is not our optimal minimum value. We need to go further down until we\n",
    "reach the minimum value. i.e x= 0. So for some number n of iterations, we have to repeat\n",
    "the same process until we reach the minimal point. That is,\n",
    "\n",
    "$$x=x-\\alpha \\cdot \\frac{d y}{d x}$$\n",
    "\n",
    "\n",
    "Okay, why is there minus in the above equation? That is why are subtracting x from the $\\alpha \\cdot \\frac{d y}{d x}$ Why can't we add them and have our equation as $x=x+\\alpha \\cdot \\frac{d y}{d x}$?\n",
    "\n",
    "\n",
    "Because we are finding the minimum of a function so we need to go downwards. If we add\n",
    "x with $\\alpha . \\frac{d y}{d x}$, then we go upwards on every iteration and we cannot find the minimum\n",
    "value as shown in the following figure:\n",
    "\n",
    "![image](images/6.png)\n",
    "\n",
    "Thus, on every iteration, we compute gradients of y with respect to x i.e $\\frac{d y}{d x}$ multiply the\n",
    "gradients with learning rate i.e $\\alpha . \\frac{d y}{d x}$ and subtract it from x value and get the new updated x value:\n",
    "\n",
    "$$x=x-\\alpha \\cdot \\frac{d y}{d x}$$\n",
    "\n",
    "By repeating this step on every iteration, we go downwards on our cost function and reach\n",
    "the minimum point. As shown in the below figure, from initial position 0.7 we reached 0.49\n",
    "and then from there we reached 0.2 then after several iterations we reached the minimum\n",
    "point 0.\n",
    "\n",
    "![image](images/7.png)\n",
    "\n",
    "\n",
    "We say we attained convergence when we reach the minimum of the function. But the\n",
    "question is, how do we know that we attained convergence? In our example, $y=x^{2}$,  we\n",
    "know that the minimum value is 0. So when we reached 0 we said that we found the\n",
    "minimum value. i.e attained the convergence. But how can we mathematically say that 0 is the minimum value of the function  $y=x^{2}$? \n",
    "\n",
    "\n",
    "Let us take a closer look at the below graph which shows how the value of x is getting\n",
    "changed on every iteration. As you may notice, the value of x is 0.009 in the 5th iteration\n",
    "and it is 0.008 in 6th iteration and it is 0.007 in 7th iteration. If you see there is no huge\n",
    "difference between all these iterations 5, 6 and 7. When there a is very less change in x value\n",
    "over iterations, then we can conclude that we attained convergence.\n",
    "\n",
    "![image](images/8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Okay, but what is the use of all this? Why are we trying to find the minimum of a\n",
    "function? When training a model our goal is to minimize the loss function of the\n",
    "model. Thus, with gradient descent, we can find the minimum of the cost function. Finding\n",
    "the minimum of the cost function gives us an optimal parameter of the model with which\n",
    "we can obtain the minimal loss. In general, we denote the parameters of the model  $\\theta$. The\n",
    "below equation is called the parameter update rule or weight update rule. \n",
    "\n",
    "$$ \\theta=\\theta-\\alpha \\cdot \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "\n",
    "Where, \n",
    "\n",
    "* $ \\theta $ is the parameter of the model\n",
    "* $\\alpha$ is the learning rate\n",
    "* $\\nabla_{\\theta} J(\\theta)$ is the gradients of loss function $J$ with respect to parameter $\\theta$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
