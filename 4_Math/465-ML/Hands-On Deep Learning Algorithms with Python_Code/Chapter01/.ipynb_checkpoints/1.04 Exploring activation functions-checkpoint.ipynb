{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring activation functions\n",
    "\n",
    "\n",
    "An activation function, also known as a transfer function, plays a vital role in neural\n",
    "networks. It is used to introduce non-linearity in neural networks. As we learned before, we\n",
    "apply the activation function to the input, which is multiplied by weights and added to the\n",
    "bias, that is, $f(z)$ , where z = (input * weights) + bias and $f(\\cdot)$ is the activation function. If we do\n",
    "not apply the activation function, then a neuron simply resembles the linear regression. The\n",
    "aim of the activation function is to introduce a non-linear transformation to learn the\n",
    "complex underlying patterns in the data.\n",
    "\n",
    "Now let's look at some of the interesting commonly used activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sigmoid function\n",
    "\n",
    "The sigmoid function is one of the most commonly used activation functions. It scales the\n",
    "value between 0 and 1. The sigmoid function can be defined as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$f(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "It is an S-shaped curve as shown below:\n",
    "\n",
    "![images](images/6.png)\n",
    "\n",
    "It is differentiable, meaning that we can find the slope of the curve at any two points. It is\n",
    "monotonic, which implies it is either entirely non-increasing or non-decreasing. The\n",
    "sigmoid function is also known as a logistic function. As we know that probability lies\n",
    "between 0 and 1 and since the sigmoid function squashes the value between 0 and 1, it is\n",
    "used for predicting the probability of output.\n",
    "The sigmoid function can be defined in python as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tanh function\n",
    "\n",
    "A hyperbolic tangent (tanh) function outputs the value between -1 to +1 and is expressed\n",
    "as follows: \n",
    "    \n",
    "$$f(x)=\\frac{1-e^{-2 x}}{1+e^{-2 x}} $$\n",
    "    \n",
    "It also resembles the S-shaped curve. Unlike a sigmoid function which is centered on 0.5,\n",
    "the tanh function is 0 centered, as shown in the following diagram:\n",
    "    \n",
    "![images](images/7.png)\n",
    "    \n",
    "Similar to the sigmoid function, it is also a differentiable and monotonic function. The tanh\n",
    "function is implemented as follows:\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    numerator = 1-np.exp(-2*x)\n",
    "    denominator = 1+np.exp(-2*x)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Unit function\n",
    "\n",
    "The Rectified Linear Unit (ReLU) function is another one of the most commonly used\n",
    "activation functions. It outputs a value from o to infinity. It is basically a piecewise function\n",
    "and can be expressed as follows: \n",
    "    \n",
    "\n",
    "$$ f(x)=\\left\\{\\begin{array}{ll}{0} & {\\text { for } x<0} \\\\ {x} & {\\text { for } x \\geq 0}\\end{array}\\right.$$\n",
    "               \n",
    "               \n",
    "That is, $f(x)$ returns zero when the value of x is less than zero and $f(x)$ returns x when the\n",
    "value of x is greater than or equal to zero. It can also be expressed as follows: \n",
    "\n",
    "$$ f(x)=\\max (0, x)$$\n",
    "\n",
    "The ReLU function is shown in the following figure:\n",
    "\n",
    "![images](images/8.png)\n",
    "\n",
    "As we can see in the preceding diagram, when we feed any negative input to the ReLU\n",
    "function, it converts it to zero. The snag for being zero for all negative values is a problem\n",
    "called dying ReLU, and a neuron is said to be dead if it always outputs zero. A ReLU\n",
    "function can be implemented as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The leaky ReLU function\n",
    "\n",
    "Leaky ReLU is a variant of the ReLU function that solves the dying ReLU problem. Instead\n",
    "of converting every negative input to zero, it has a small slope for a negative value as\n",
    "shown:\n",
    "\n",
    "![images](images/9.png)\n",
    "\n",
    "Leaky ReLU can be expressed as follows:\n",
    "\n",
    "$$ f(x)=\\left\\{\\begin{array}{ll}{\\alpha x} & {\\text { for } x<0} \\\\ {x} & {\\text { for } x \\geq 0}\\end{array}\\right.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leakyReLU(x,alpha=0.01):\n",
    "    if x<0:\n",
    "        return (alpha*x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " The value  $\\alpha$ of is typically set to 0.01. The leaky ReLU function is implemented as follows:\n",
    "Instead of setting some default values to $\\alpha$, we can send them as a parameter to a neural\n",
    "network and make the network learn the optimal value of  $\\alpha$. Such an activation function\n",
    "can be termed as a Parametric ReLU function. We can also set the value of  $\\alpha$ to some\n",
    "random value and it is called as Randomized ReLU function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Exponential linear unit function\n",
    "\n",
    "Exponential linear unit (ELU), like Leaky ReLU, has a small slope for negative values. But\n",
    "instead of having a straight line, it has a log curve, as shown in the following diagram:\n",
    "   \n",
    "![images](images/10.png)\n",
    "    \n",
    "It can be expressed as follows:\n",
    "    \n",
    "$$ f(x)=\\left\\{\\begin{array}{ll}{\\alpha\\left(e^{x}-1\\right)} & {\\text { for } x<0} \\\\ {x} & {\\text { for } x \\geq 0}\\end{array}\\right.$$\n",
    "The ELU function is implemented in python as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELU(x,alpha=0.01):\n",
    "    if x<0:\n",
    "        return (alpha*(np.exp(x)-1))\n",
    "\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Swish function\n",
    "The Swish function is a recently introduced activation function by Google. Unlike other\n",
    "activation functions, which are monotonic, Swish is a non-monotonic function, which\n",
    "means it is neither always non-increasing nor non-decreasing. It provides better\n",
    "performance than ReLU. It is simple and can be expressed as follows:\n",
    "    \n",
    "\n",
    "$$f(x)=x \\sigma(x)$$\n",
    "\n",
    "Here, $\\sigma(x)$ is the sigmoid function. The Swish function is shown in the following diagram:\n",
    "\n",
    "![images](images/11.png)\n",
    "    \n",
    "    \n",
    "We can also reparametrize the Swish function and express it as follows:\n",
    "    \n",
    "$$f(x)=2 x \\sigma(\\beta x)$$\n",
    "\n",
    "\n",
    "When the value of $\\beta$ is 0, then we get the identity function $f(x) = x$.\n",
    "It becomes a linear function and, when the value of $\\beta$ tends to infinity, then $f(x)$ becomes\n",
    "$ 2max (0, x)$, which is basically the ReLU function multiplied by some constant value. So, the\n",
    "value of $\\beta$ acts as a good interpolation between a linear and a nonlinear function. The swish\n",
    "function can be implemented as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish(x,beta):\n",
    "    return 2*x*sigmoid(beta*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The softmax function\n",
    "\n",
    "\n",
    "The softmax function is basically the generalization of the sigmoid function. It is usually\n",
    "applied to the final layer of the network and while performing multi-class classification\n",
    "tasks. It gives the probabilities of each class for being output and thus, the sum of softmax\n",
    "values will always equal 1.\n",
    "\n",
    "It can be represented as follows:\n",
    "$$f\\left(x_{i}\\right)=\\frac{e^{x_{i}}}{\\sum_{j} e^{x_{j}}}$$\n",
    "\n",
    "As shown in the following diagram, the softmax function converts their inputs to\n",
    "probabilities:\n",
    "    \n",
    "![images](images/12.png)\n",
    "    \n",
    "    \n",
    "    The softmax function can be implemented in python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will how neural network uses forward propagation to predict output. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
