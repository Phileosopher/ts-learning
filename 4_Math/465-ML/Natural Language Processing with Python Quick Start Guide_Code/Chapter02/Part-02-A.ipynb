{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'\n",
    "file_name = 'sherlock.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(file_name, 'wb') as out_file:\n",
    "        data = response.read() # a `bytes` object\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt\n",
      "sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "!ls {*.txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -2 sherlock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -i 1,33d sherlock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "\n",
      "by\n",
      "\n",
      "SIR ARTHUR CONAN DOYLE\n"
     ]
    }
   ],
   "source": [
    "!head -5 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE A\n"
     ]
    }
   ],
   "source": [
    "#let's the load data to RAM\n",
    "text = open(file_name, 'r', encoding='utf-8').read()  # note that I add an encoding='utf-8' parameter to preserve information\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is loaded as datatype: <class 'str'> and has 581204 characters in it\n"
     ]
    }
   ],
   "source": [
    "print(f'The file is loaded as datatype: {type(text)} and has {len(text)} characters in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n",
      "There are 85 unique characters, including both ASCII and Unicode character\n"
     ]
    }
   ],
   "source": [
    "# how many unique characters do we see? \n",
    "# For reference, ASCII has 127 characters in it - so we expect this to have at most 127 characters\n",
    "unique_chars = list(set(text))\n",
    "unique_chars.sort()\n",
    "print(unique_chars)\n",
    "print(f'There are {len(unique_chars)} unique characters, including both ASCII and Unicode character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "### Split by Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107431\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler.', 'All', 'emotions,', 'and', 'that', 'one', 'particularly,', 'were', 'abhorrent', 'to', 'his', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'He', 'was,', 'I', 'take', 'it,', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen,', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions,', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer.', 'They', 'were', 'admirable', 'things', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[90:200])  #start with the first chapeter, ignoring the index for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red-headed', 'woman', 'on', 'the', 'street']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at another example: \n",
    "'red-headed woman on the street'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Word Extraction\n",
    "**Introducing Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words', 'words', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\W+', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_alphanumeric = re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109111, 107431)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_alphanumeric), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOHEMIA', 'I', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', 'All', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'He', 'was', 'I', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'They', 'were', 'admirable']\n"
     ]
    }
   ],
   "source": [
    "print(words_alphanumeric[90:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isn', 't', 'he', 'coming', 'home', 'for', 'dinner', 'with', 'the', 'red', 'headed', 'girl', '']\n"
     ]
    }
   ],
   "source": [
    "words_break = re.split('\\W+', \"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print(words_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[whole, of, her, sex, ., It, was, not, that, he, felt, \n",
      ", any, emotion, akin, to, love, for, Irene, Adler, ., All, emotions, ,, and, that, \n",
      ", one, particularly, ,, were, abhorrent, to, his, cold, ,, precise, but, \n",
      ", admirably, balanced, mind, ., He, was, ,, I, take, it, ,]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc)[150:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, spaCy tokenizes all *punctuations and words* and returned those as individual tokens as well. Let's try the example which we didn't like earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Is, n't, he, coming, home, for, dinner, with, the, red, -, headed, girl, ?]\n"
     ]
    }
   ],
   "source": [
    "words = nlp(\"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print([token for token in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes, she is always THE woman., I have seldom heard\n",
      "him mention her under any other name., In his eyes she eclipses\n",
      "and predominates the whole of her sex., It was not that he felt\n",
      "any emotion akin to love for Irene Adler.]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences[13:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP WORD REMOVAL & CASE CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has already marked each token as a stop word or not and stored it in `is_stop` attribute of each token. This makes it very handy for text cleaning. Let's take a quick look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_example = \"the AI/AGI uprising cannot happen without the progress of NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, True, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, True, False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE False False\n",
      "ADVENTURES False False\n",
      "OF False False\n",
      "SHERLOCK False False\n",
      "HOLMES False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:5]:\n",
    "    print(token, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()  # native python function\n",
    "doc_lower = nlp(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the True\n",
      "adventures False\n",
      "of True\n",
      "sherlock False\n",
      "holmes False\n"
     ]
    }
   ],
   "source": [
    "for token in doc_lower[:5]:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spaCy has a dictionary of 305 stop words'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "f'spaCy has a dictionary of {len(list(STOP_WORDS))} stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_stop_words = [\"NLP\", \"Processing\", \"AGI\"]\n",
    "for word in domain_stop_words:\n",
    "    STOP_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, True, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, True, False)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'uprising', 'happen', 'progress']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', '/', 'uprising', 'happen', 'progress']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy for Lemmatization\n",
    "**spaCy only supports lemmatization** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An underscore at end, such as `lemma_` tells spaCy we are looking for something which is human readable. spaCy stores the internal hash or identifier which spaCy stores in `token.lemma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Their, '-PRON-', 561228191312463089, 'ADJ'),\n",
       " (Apples, 'apples', 14374618037326464786, 'PROPN'),\n",
       " (&, '&', 15473034735919704609, 'CCONJ'),\n",
       " (Banana, 'banana', 2525716904149915114, 'PROPN'),\n",
       " (fruit, 'fruit', 17674554054627885835, 'NOUN'),\n",
       " (salads, 'salad', 16382906660984395826, 'NOUN'),\n",
       " (are, 'be', 10382539506755952630, 'VERB'),\n",
       " (amazing, 'amazing', 12968186374132960503, 'ADJ'),\n",
       " (., '.', 12646065887601541794, 'PUNCT'),\n",
       " (Would, 'would', 6992604926141104606, 'VERB'),\n",
       " (you, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (like, 'like', 18194338103975822726, 'VERB'),\n",
       " (meeting, 'meet', 6880656908171229526, 'VERB'),\n",
       " (me, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (at, 'at', 11667289587015813222, 'ADP'),\n",
       " (the, 'the', 7425985699627899538, 'DET'),\n",
       " (cafe, 'cafe', 10569699879655997926, 'NOUN'),\n",
       " (?, '?', 8205403955989537350, 'PUNCT')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentence_example = \"Their Apples & Banana fruit salads are amazing. Would you like meeting me at the cafe?\"\n",
    "[(token, token.lemma_, token.lemma, token.pos_ ) for token in nlp(lemma_sentence_example)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
