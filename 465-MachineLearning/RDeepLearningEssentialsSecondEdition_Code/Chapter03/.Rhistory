?install.packages
uninstall.packages("caret")
install.packages("installr")
library("installr")
uninstall.packages("caret")
devtools::install_github('topepo/caret/pkg/caret')
library("caret")
library(caret)
library(caret)
install.packages("C:/Users/ML_PC/Downloads/caret_6.0-78.tar.gz", repos = NULL)
library(caret)
library(glmnet)
library(MASS)
library(parallel)
library(foreach)
library(doSNOW)
options(width = 70, digits = 2)
dataDirectory <- "../data"
if (!file.exists(paste(dataDirectory,'/train.csv',sep="")))
{
link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'
if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))
download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))
unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)
if (file.exists(paste(dataDirectory,'/test.csv',sep="")))
file.remove(paste(dataDirectory,'/test.csv',sep=""))
}
## same data as from previous chapter
digits.train <- read.csv("../data/train.csv")
## convert to factor
digits.train$label <- factor(digits.train$label, levels = 0:9)
sample <- sample(nrow(digits.train), 6000)
train <- sample[1:5000]
test <- sample[5001:6000]
digits.X <- digits.train[train, -1]
digits.y <- digits.train[train, 1]
## try various weight decays and number of iterations
## register backend so that different decays can be
## estimated in parallel
cl <- makeCluster(parallel::detectCores() -1)
clusterEvalQ(cl, {source("cluster_inc.R")})
registerDoSNOW(cl)
set.seed(1234)
digits.decay.m1 <- lapply(c(100, 150), function(its) {
caret::train(digits.X, digits.y,
method = "nnet",
tuneGrid = expand.grid(
.size = c(10),
.decay = c(0, .1)),
trControl = caret::trainControl(method="cv", number=5, repeats=1),
MaxNWts = 10000,
maxit = its)
})
digits.decay.m1[[1]]
digits.decay.m1[[2]]
## simulated data
set.seed(1234)
d <- data.frame(
x = rnorm(400))
d$y <- with(d, rnorm(400, 2 + ifelse(x < 0, x + x^2, x + x^2.5), 1))
d.train <- d[1:200, ]
d.test <- d[201:400, ]
## three different models
m1 <- lm(y ~ x, data = d.train)
m2 <- lm(y ~ I(x^2), data = d.train)
m3 <- lm(y ~ pmax(x, 0) + pmin(x, 0), data = d.train)
## In sample R2
cbind(M1=summary(m1)$r.squared,
M2=summary(m2)$r.squared,M3=summary(m3)$r.squared)
options(width = 70, digits = 2)
## simulated data
set.seed(1234)
d <- data.frame(
x = rnorm(400))
d$y <- with(d, rnorm(400, 2 + ifelse(x < 0, x + x^2, x + x^2.5), 1))
d.train <- d[1:200, ]
d.test <- d[201:400, ]
## three different models
m1 <- lm(y ~ x, data = d.train)
m2 <- lm(y ~ I(x^2), data = d.train)
m3 <- lm(y ~ pmax(x, 0) + pmin(x, 0), data = d.train)
## In sample R2
cbind(M1=summary(m1)$r.squared,
M2=summary(m2)$r.squared,M3=summary(m3)$r.squared)
## correlations in the training data
cor(cbind(M1=fitted(m1),
M2=fitted(m2),M3=fitted(m3)))
## generate predictions and the average prediction
d.test$yhat1 <- predict(m1, newdata = d.test)
d.test$yhat2 <- predict(m2, newdata = d.test)
d.test$yhat3 <- predict(m3, newdata = d.test)
d.test$yhatavg <- rowMeans(d.test[, paste0("yhat", 1:3)])
## correlation in the testing data
cor(d.test)
## Fit Models
nn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {
set.seed(1234)
list(nn.train(
x = as.matrix(digits.X),
y = model.matrix(~ 0 + digits.y),
hidden = c(40, 80, 40, 80)[i],
activationfun = "tanh",
learningrate = 0.8,
momentum = 0.5,
numepochs = 150,
output = "softmax",
hidden_dropout = c(0, 0, .5, .5)[i],
visible_dropout = c(0, 0, .2, .2)[i]))
}
library(deepnet)
install.packages("deepnet")
library(deepnet)
## Fit Models
nn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {
set.seed(1234)
list(nn.train(
x = as.matrix(digits.X),
y = model.matrix(~ 0 + digits.y),
hidden = c(40, 80, 40, 80)[i],
activationfun = "tanh",
learningrate = 0.8,
momentum = 0.5,
numepochs = 150,
output = "softmax",
hidden_dropout = c(0, 0, .5, .5)[i],
visible_dropout = c(0, 0, .2, .2)[i]))
}
?nn.train
library(glmnet)
library(MASS)
library(parallel)
library(foreach)
library(doSNOW)
options(width = 70, digits = 2)
dataDirectory <- "../data"
if (!file.exists(paste(dataDirectory,'/train.csv',sep="")))
{
link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'
if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))
download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))
unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)
if (file.exists(paste(dataDirectory,'/test.csv',sep="")))
file.remove(paste(dataDirectory,'/test.csv',sep=""))
}
## same data as from previous chapter
digits.train <- read.csv("../data/train.csv")
## convert to factor
digits.train$label <- factor(digits.train$label, levels = 0:9)
sample <- sample(nrow(digits.train), 6000)
train <- sample[1:5000]
test <- sample[5001:6000]
digits.X <- digits.train[train, -1]
digits.y <- digits.train[train, 1]
## try various weight decays and number of iterations
## register backend so that different decays can be
## estimated in parallel
cl <- makeCluster(parallel::detectCores() -1)
clusterEvalQ(cl, {source("cluster_inc.R")})
registerDoSNOW(cl)
## Fit Models
nn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {
set.seed(1234)
list(nn.train(
x = as.matrix(digits.X),
y = model.matrix(~ 0 + digits.y),
hidden = c(40, 80, 40, 80)[i],
activationfun = "tanh",
learningrate = 0.8,
momentum = 0.5,
numepochs = 150,
output = "softmax",
hidden_dropout = c(0, 0, .5, .5)[i],
visible_dropout = c(0, 0, .2, .2)[i]))
}
nn.yhat <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))
})
library( RSNNS)
nn.yhat <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))
})
library(deepnet)
nn.yhat <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))
})
perf.train <- do.call(cbind, lapply(nn.yhat, function(yhat) {
caret::confusionMatrix(xtabs(~ I(yhat - 1) + digits.y))$overall
}))
colnames(perf.train) <- c("N40", "N80", "N40_Reg", "N80_Reg")
options(digits = 4)
perf.train
test.X <- digits.train[test, -1]
test.y <- digits.train[test, 1]
nn.yhat.test <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(test.X)))
})
perf.test <- do.call(cbind, lapply(nn.yhat.test, function(yhat) {
caret::confusionMatrix(xtabs(~ I(yhat - 1) + test.y))$overall
}))
colnames(perf.test) <- c("N40", "N80", "N40_Reg", "N80_Reg")
perf.test
shiny::runApp()
runApp()
round(0.576575,3)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?sidebarPanel
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?column
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
dfCost <- data.frame(epoch=integer,cost=double())
dfCost <- data.frame(epoch=integer(),cost=double())
dfCost <- rbind(dfCost,c(1,0.33))
dfCost
runApp()
runApp()
runApp()
runApp()
runApp()
?plot
runApp()
runApp()
runApp()
runApp()
?column
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
source("nnet_functions.R")
data_sel <- "bulls_eye"
df <- getData(data_sel)
model <- glm(Y ~.,family=binomial(link='logit'),data=df)
res <- predict(model,newdata=df,type='response')
res <- ifelse(res > 0.5,1,0)
df$Y2<-res
logreg_accuracy <- sum(df$Y==df$Y2) / nrow(df)
plot(df[,1:2],col=rainbow(2)[(1+df$Y)],main=paste("Logistic regression accuracy on",data_sel,"=",logreg_accuracy))
####################### neural network ######################
hidden <- 3
epochs <- 3000
lr <- 0.5
activation_ftn <- "sigmoid"
#activation_ftn <- "tanh"
#activation_ftn <- "relu"
df <- getData(data_sel) # from nnet_functions
X <- as.matrix(df[,1:2])
Y <- as.matrix(df$Y)
n_x=ncol(X)
n_h=hidden
n_y=1
m <- nrow(X)
# initialise weights
set.seed(42)
weights1 <- matrix(0.01*runif(n_h*n_x)-0.005, ncol=n_x, nrow=n_h)
weights2 <- matrix(0.01*runif(n_y*n_h)-0.005, ncol=n_h, nrow=n_y)
bias1 <- matrix(rep(0,n_h),nrow=n_h,ncol=1)
bias2 <- matrix(rep(0,n_y),nrow=n_y,ncol=1)
for (i in 0:epochs)
{
activation2 <- forward_prop(t(X),activation_ftn,weights1,bias1,weights2,bias2)
cost <- cost_f(activation2,t(Y))
backward_prop(t(X),t(Y),activation_ftn,weights1,weights2,activation1,activation2)
weights1 <- weights1 - (lr * dweights1)
bias1 <- bias1 - (lr * dbias1)
weights2 <- weights2 - (lr * dweights2)
bias2 <- bias2 - (lr * dbias2)
if ((i %% 500) == 0)
print (paste(" Cost after",i,"epochs =",cost))
}
################################################################################
##                                                                            ##
##                                  Setup                                     ##
##                                                                            ##
################################################################################
library(glmnet)
library(MASS)
library(deepnet)
library(RSNNS)
library(parallel)
library(foreach)
library(doSNOW)
options(width = 70, digits = 2)
################################################################################
##                                                                            ##
##                                L1 Penalty                                  ##
##                                                                            ##
################################################################################
## set seed and simulate data
set.seed(1234)
X <- mvrnorm(n = 200, mu = c(0, 0, 0, 0, 0),
Sigma = matrix(c(
1, .9999, .99, .99, .10,
.9999, 1, .99, .99, .10,
.99, .99, 1, .99, .10,
.99, .99, .99, 1, .10,
.10, .10, .10, .10, 1
), ncol = 5))
y <- rnorm(200, 3 + X %*% matrix(c(1, 1, 1, 1, 0)), .5)
m.ols <- lm(y[1:100] ~ X[1:100, ])
m.lasso.cv <- cv.glmnet(X[1:100, ], y[1:100], alpha = 1)
plot(m.lasso.cv)
cbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1])
m.ridge.cv <- cv.glmnet(X[1:100, ], y[1:100], alpha = 0)
plot(m.ridge.cv)
cbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1],Ridge = coef(m.ridge.cv)[,1])
if (!file.exists('../data/train.csv'))
{
link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'
if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))
download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))
unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)
if (file.exists(paste(dataDirectory,'/test.csv',sep="")))
file.remove(paste(dataDirectory,'/test.csv',sep=""))
}
## same data as from previous chapter
digits.train <- read.csv("../data/train.csv")
## convert to factor
digits.train$label <- factor(digits.train$label, levels = 0:9)
sample <- sample(nrow(digits.train), 6000)
train <- sample[1:5000]
test <- sample[5001:6000]
set.seed(42)
set.seed(42)
if (!file.exists('../data/train.csv'))
{
link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'
if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))
download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))
unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)
if (file.exists(paste(dataDirectory,'/test.csv',sep="")))
file.remove(paste(dataDirectory,'/test.csv',sep=""))
}
## same data as from previous chapter
digits.train <- read.csv("../data/train.csv")
## convert to factor
digits.train$label <- factor(digits.train$label, levels = 0:9)
sample <- sample(nrow(digits.train), 6000)
train <- sample[1:5000]
test <- sample[5001:6000]
digits.X <- digits.train[train, -1]
digits.y <- digits.train[train, 1]
test.X <- digits.train[test, -1]
test.y <- digits.train[test, 1]
## try various weight decays and number of iterations
## register backend so that different decays can be
## estimated in parallel
cl <- makeCluster(parallel::detectCores() -1)
clusterEvalQ(cl, {source("cluster_inc.R")})
registerDoSNOW(cl)
set.seed(1234)
digits.decay.m1 <- lapply(c(200, 300), function(its) {
caret::train(digits.X, digits.y,
method = "nnet",
tuneGrid = expand.grid(
.size = c(10),
.decay = c(0, .1)),
trControl = caret::trainControl(method="cv", number=5, repeats=1),
MaxNWts = 10000,
maxit = its)
})
digits.decay.m1[[1]]
digits.decay.m1[[2]]
## simulated data
set.seed(1234)
d <- data.frame(
x = rnorm(400))
d$y <- with(d, rnorm(400, 2 + ifelse(x < 0, x + x^2, x + x^2.5), 1))
d.train <- d[1:200, ]
d.test <- d[201:400, ]
## three different models
m1 <- lm(y ~ x, data = d.train)
m2 <- lm(y ~ I(x^2), data = d.train)
m3 <- lm(y ~ pmax(x, 0) + pmin(x, 0), data = d.train)
## In sample R2
cbind(M1=summary(m1)$r.squared,
M2=summary(m2)$r.squared,M3=summary(m3)$r.squared)
## correlations in the training data
cor(cbind(M1=fitted(m1),
M2=fitted(m2),M3=fitted(m3)))
## generate predictions and the average prediction
d.test$yhat1 <- predict(m1, newdata = d.test)
d.test$yhat2 <- predict(m2, newdata = d.test)
d.test$yhat3 <- predict(m3, newdata = d.test)
d.test$yhatavg <- rowMeans(d.test[, paste0("yhat", 1:3)])
## correlation in the testing data
cor(d.test)
## Fit Models
nn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {
set.seed(1234)
list(nn.train(
x = as.matrix(digits.X),
y = model.matrix(~ 0 + digits.y),
hidden = c(40, 80, 40, 80)[i],
activationfun = "tanh",
learningrate = 0.8,
momentum = 0.5,
numepochs = 150,
output = "softmax",
hidden_dropout = c(0, 0, .5, .5)[i],
visible_dropout = c(0, 0, .2, .2)[i]))
}
nn.yhat <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))
})
perf.train <- do.call(cbind, lapply(nn.yhat, function(yhat) {
caret::confusionMatrix(xtabs(~ I(yhat - 1) + digits.y))$overall
}))
colnames(perf.train) <- c("N40", "N80", "N40_Reg", "N80_Reg")
options(digits = 4)
perf.train
## Fit Models
nn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {
set.seed(1234)
list(nn.train(
x = as.matrix(digits.X),
y = model.matrix(~ 0 + digits.y),
hidden = c(40, 80, 40, 80)[i],
activationfun = "tanh",
learningrate = 0.8,
momentum = 0.5,
numepochs = 150,
output = "softmax",
hidden_dropout = c(0, 0, .5, .5)[i],
visible_dropout = c(0, 0, .2, .2)[i]))
}
nn.yhat <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))
})
perf.train <- do.call(cbind, lapply(nn.yhat, function(yhat) {
caret::confusionMatrix(xtabs(~ I(yhat - 1) + digits.y))$overall
}))
colnames(perf.train) <- c("N40", "N80", "N40_Reg", "N80_Reg")
options(digits = 4)
perf.train
nn.yhat.test <- lapply(nn.models, function(obj) {
encodeClassLabels(nn.predict(obj, as.matrix(test.X)))
})
perf.test <- do.call(cbind, lapply(nn.yhat.test, function(yhat) {
caret::confusionMatrix(xtabs(~ I(yhat - 1) + test.y))$overall
}))
colnames(perf.test) <- c("N40", "N80", "N40_Reg", "N80_Reg")
perf.test
