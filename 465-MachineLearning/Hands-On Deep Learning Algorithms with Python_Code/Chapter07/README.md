
# [Chapter 7. Learning Text Representations](#)

* [7.1. Understanding Word2vec Model](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/07.%20Learning%20Text%20Representations/7.01%20Understanding%20Word2vec%20Model.ipynb)
* [7.2. Continuous Bag of words](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/07.%20Learning%20Text%20Representations/7.02%20Continuous%20Bag%20of%20words.ipynb)
* 7.3. Math of CBOW
	* 7.3.1. Deriving Forward Propagation
	* 7.3.2. Deriving Backward Propagation
* 7.4. Skip- Gram model
* 7.5. Math of Skip-Gram 
	* 7.5.1. Deriving Forward Propagation
	* 7.5.2. Deriving Backward Propagation
* 7.6. various training strategies
	* 7.6.1. Hierarchical Softmax
	* 7.6.2. Negative sampling
	* 7.6.3. Subsampling frequent words
*  [ 7.7. Building word2vec model using Gensim](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/07.%20Learning%20Text%20Representations/7.07%20Building%20word2vec%20model%20using%20Gensim.ipynb)
*  [7.8. Visualizing word embeddings in TensorBoard](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/07.%20Learning%20Text%20Representations/7.08%20Visualizing%20Word%20Embeddings%20in%20TensorBoard.ipynb)
* 7.9. Converting documents to vectors using doc2vec
	* 7.9.1. PV-DM
	* 7.9.2. PV-DBOW
*  [7.10. Finding similar documents using Doc2vec](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/07.%20Learning%20Text%20Representations/7.10%20Finding%20similar%20documents%20using%20Doc2Vec.ipynb)
* 7.11. Understanding skip thoughts algorithm
* 7.12 Quick thoughts for sentence embeddings
