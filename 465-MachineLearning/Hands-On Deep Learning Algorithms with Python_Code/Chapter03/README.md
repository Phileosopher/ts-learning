
# [Chapter 3. Gradient Descent and its variants](#)

* [3.1. Demystifying Gradient Descent](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/03.%20Gradient%20Descent%20and%20its%20variants/3.01%20Demystifying%20Gradient%20Descent.ipynb)
* [3.2. Performing Gradient Descent in Regression](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/03.%20Gradient%20Descent%20and%20its%20variants/3.02%20Performing%20Gradient%20Descent%20in%20Regression.ipynb)
* 3.3. Gradient Descent vs Stochastic Gradient Descent
* 3.4. Momentum based  Gradient Descent
	* 3.4.1. Gradient Descent with Momentum
	* 3.4.2. Nesterov Accelerated Gradient
* 3.5. Adaptive methods of Gradient Descent
	* 3.5.1. Set Learning rate adaptively using AdaGrad
	* 3.5.2. Do away with Learning rate using AdaDelta
	* 3.5.3. Overcoming limitations of AdaGrad using RMSProp
	* 3.5.4. Adam - Adaptive Moment Estimation
	* 3.5.6. Adamax - Adam based on infinity norm
	* 3.5.7. Adaptive Moment Estimation with AMSGrad
	* 3.5.8. Nadam - Adding NAG to ADAM
* [ 3.6. Implementing Various Gradient descent methods from Scratch](https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/blob/master/03.%20Gradient%20Descent%20and%20its%20variants/3.06%20%20Implementing%20Several%20Variants%20of%20Gradient%20Descent%20from%20Scratch.ipynb)
