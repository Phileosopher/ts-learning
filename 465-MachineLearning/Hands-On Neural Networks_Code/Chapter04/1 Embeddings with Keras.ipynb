{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:02:30.422017Z",
     "start_time": "2019-05-26T11:02:25.541712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is good pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love Italian pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The best pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nice pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The pizza was alright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>disgusting pineapple pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not good pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>very bad pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I had better pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  sentiment\n",
       "0           This is good pizza          1\n",
       "1         I love Italian pizza          1\n",
       "2               The best pizza          1\n",
       "3                   nice pizza          1\n",
       "4              Excellent pizza          1\n",
       "5                 I love pizza          1\n",
       "6        The pizza was alright          0\n",
       "7   disgusting pineapple pizza          0\n",
       "8               not good pizza          0\n",
       "9                    bad pizza          0\n",
       "10              very bad pizza          0\n",
       "11          I had better pizza          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# define the corpus\n",
    "corpus = ['This is good pizza',\n",
    "        'I love Italian pizza',\n",
    "        'The best pizza',\n",
    "        'nice pizza',\n",
    "        'Excellent pizza',\n",
    "        'I love pizza',\n",
    "        'The pizza was alright',\n",
    "        'disgusting pineapple pizza',\n",
    "        'not good pizza',\n",
    "        'bad pizza',\n",
    "        'very bad pizza',\n",
    "        'I had better pizza']\n",
    "\n",
    "\n",
    "# creating class labels for our \n",
    "labels = array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "output_dim = 8\n",
    "pd.DataFrame({'text': corpus, 'sentiment':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:02:30.436345Z",
     "start_time": "2019-05-26T11:02:30.425888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 17, 17, 15],\n",
       " [6, 2, 7, 15],\n",
       " [3, 5, 15],\n",
       " [2, 15],\n",
       " [16, 15],\n",
       " [6, 2, 15],\n",
       " [3, 15, 18, 14],\n",
       " [14, 10, 15],\n",
       " [18, 17, 15],\n",
       " [12, 15],\n",
       " [15, 12, 15],\n",
       " [6, 9, 10, 15]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we extract the vocabulary from our corpus\n",
    "sentences = [voc.split() for voc in corpus]\n",
    "vocabulary = set([word for sentence in sentences for word in sentence])\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "encoded_corpus = [one_hot(d, vocab_size) for d in corpus]\n",
    "encoded_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:02:30.447124Z",
     "start_time": "2019-05-26T11:02:30.439663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 17 17 15  0]\n",
      " [ 6  2  7 15  0]\n",
      " [ 3  5 15  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [16 15  0  0  0]\n",
      " [ 6  2 15  0  0]\n",
      " [ 3 15 18 14  0]\n",
      " [14 10 15  0  0]\n",
      " [18 17 15  0  0]\n",
      " [12 15  0  0  0]\n",
      " [15 12 15  0  0]\n",
      " [ 6  9 10 15  0]]\n"
     ]
    }
   ],
   "source": [
    "# we now pad the documents to  \n",
    "# the max length of the longest sentences\n",
    "# to have an uniform length\n",
    "max_length = 5\n",
    "padded_docs = pad_sequences(encoded_corpus, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:02:32.406902Z",
     "start_time": "2019-05-26T11:02:30.452206Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 8)              160       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 91.666669\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim, input_length=max_length, name='embedding'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:01:56.871276Z",
     "start_time": "2019-05-26T11:01:56.864585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:09:24.472123Z",
     "start_time": "2019-05-26T11:09:22.283363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 7], [5, 1], [9, 1], [2, 1], [4], [2], [1, 1], [4, 5], [1, 1], [2, 5, 7, 3]]\n",
      "[[5 7 0 0]\n",
      " [5 1 0 0]\n",
      " [9 1 0 0]\n",
      " [2 1 0 0]\n",
      " [4 0 0 0]\n",
      " [2 0 0 0]\n",
      " [1 1 0 0]\n",
      " [4 5 0 0]\n",
      " [1 1 0 0]\n",
      " [2 5 7 3]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              80        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 113\n",
      "Trainable params: 113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 60.000002\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "\n",
    "vocabulary = set(docs)\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = len(set(docs))\n",
    "\n",
    "encoded_corpus = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_corpus)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_corpus, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:10:32.624221Z",
     "start_time": "2019-05-26T11:10:09.245468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marinamattos/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'beautiful', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens = nltk.word_tokenize('This is a beautiful sentence')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T11:10:33.025807Z",
     "start_time": "2019-05-26T11:10:32.644321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagget_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tagget_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
